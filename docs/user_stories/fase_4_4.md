## Refinamento Colaborativo: Hist√≥rias da Fase 3: Flexibiliza√ß√£o do Motor de Gera√ß√£o

---
### Hist√≥ria 1: Refatora√ß√£o do Servi√ßo de Gera√ß√£o para Suporte a M√∫ltiplos Provedores de LLM
* **Tipo:** T√©cnica / Habilitadora

#### Parte 1: Especifica√ß√£o Funcional (Vis√£o do Product Owner)
* **Hist√≥ria de Usu√°rio:** Como um Desenvolvedor, eu quero refatorar o servi√ßo de gera√ß√£o de texto para que ele possa utilizar diferentes provedores de LLM (Ollama, OpenAI, Google Gemini) de forma intercambi√°vel, para que o sistema se torne mais flex√≠vel e possamos otimizar custos e performance escolhendo o melhor modelo para cada cen√°rio.
* **Requisitos / Detalhes:**
    * A sele√ß√£o do provedor de LLM e de Embedding deve ser centralizada no arquivo de configura√ß√µes (`src/config/settings.py`).
    * Deve ser poss√≠vel alterar o provedor em tempo de execu√ß√£o apenas modificando a configura√ß√£o, sem a necessidade de alterar o c√≥digo-fonte da aplica√ß√£o.
    * A implementa√ß√£o atual, que usa Ollama, deve ser mantida e adaptada para funcionar dentro da nova estrutura flex√≠vel.
    * O sistema deve lan√ßar um erro claro na inicializa√ß√£o se um provedor configurado n√£o tiver uma implementa√ß√£o correspondente.
* **Crit√©rios de Aceite (ACs):**
    * **AC 1:** Dado que a configura√ß√£o em `settings.py` especifica `LLM_PROVIDER="ollama"`, quando eu envio uma requisi√ß√£o `POST` para `/query`, ent√£o o sistema continua funcionando como antes, utilizando o Ollama para gerar a resposta, e todos os testes existentes continuam passando.
    * **AC 2:** Dado que a configura√ß√£o em `settings.py` √© alterada para um provedor ainda n√£o implementado (e.g., `LLM_PROVIDER="anthropic"`), quando a aplica√ß√£o tenta iniciar, ent√£o ela deve falhar com uma mensagem de erro informativa, como "Provedor de LLM 'anthropic' n√£o suportado".
    * **AC 3:** Dado que a estrutura foi refatorada, quando um novo desenvolvedor analisa o `src/generation/`, ent√£o a forma de adicionar um novo provedor de LLM √© clara e segue um padr√£o definido (como implementar uma interface ou herdar de uma classe base).
* **Defini√ß√£o de 'Pronto' (DoD Checklist):**
    * [ ] C√≥digo revisado por um par (PR aprovado).
    * [ ] Testes de unidade para a l√≥gica de sele√ß√£o de provedor foram criados e est√£o passando.
    * [ ] Nenhuma regress√£o funcional foi introduzida (validado pelos testes de integra√ß√£o existentes).
    * [ ] A configura√ß√£o foi movida com sucesso para o arquivo `settings.py`.

#### Parte 2: Especifica√ß√£o T√©cnica (Vis√£o do Engenheiro)
* **Abordagem T√©cnica Proposta:**
    * Aplicar o padr√£o de projeto **Strategy** ou **Factory**. Criaremos uma interface (classe base abstrata) `LLMProvider` em `src/generation/` que define um contrato comum (e.g., um m√©todo `generate(prompt)`). A l√≥gica atual do Ollama ser√° movida para uma classe concreta `OllamaProvider` que implementa essa interface. Um factory ou um bloco `if/elif` no servi√ßo de gera√ß√£o ser√° respons√°vel por instanciar o provedor correto com base na configura√ß√£o.
* **Backend:**
    * - **Configura√ß√£o:** Em `src/config/settings.py`, adicionar novas vari√°veis:
        * `LLM_PROVIDER: Literal["ollama", "openai", "gemini"] = "ollama"`
        * `EMBEDDING_PROVIDER: Literal["ollama", "openai"] = "ollama"`
        * `OPENAI_API_KEY: str = ""`
        * `GOOGLE_API_KEY: str = ""`
    * - **Refatora√ß√£o:**
        * Criar `src/generation/providers/base.py` com a classe abstrata `LLMProvider`.
        * Criar `src/generation/providers/ollama.py` com a classe `OllamaProvider` que encapsula a l√≥gica existente.
        * Modificar `src/generation/generator.py` para usar o factory que seleciona e instancia o provedor com base em `settings.LLM_PROVIDER`.
    * - **Testes:** Criar `tests/test_generation_providers.py` para testar a l√≥gica do factory e garantir que ele falhe corretamente com configura√ß√µes inv√°lidas.
* **Frontend:**
    * - N/A.
* **Banco de Dados:**
    * - Nenhuma altera√ß√£o necess√°ria.
* **Quest√µes em Aberto / Riscos:**
    * - **Gerenciamento de Segredos:** As chaves de API (OpenAI, Google) n√£o devem ser commitadas. O uso de vari√°veis de ambiente (lidas pelo Pydantic) √© obrigat√≥rio. Isso deve ser documentado.
    * - **Interface Comum:** Garantir que a interface `LLMProvider` seja gen√©rica o suficiente para acomodar as diferentes respostas e par√¢metros dos SDKs da OpenAI e do Google.

---
### Hist√≥ria 2: Implementa√ß√£o do Provedor OpenAI para Gera√ß√£o e Embeddings
* **Tipo:** Funcional

#### Parte 1: Especifica√ß√£o Funcional (Vis√£o do Product Owner)
* **Hist√≥ria de Usu√°rio:** Como um Desenvolvedor, eu quero integrar os modelos da OpenAI como uma op√ß√£o para gera√ß√£o de respostas e cria√ß√£o de embeddings, para que eu possa utilizar seus modelos de ponta e avaliar a rela√ß√£o custo-benef√≠cio em compara√ß√£o com a solu√ß√£o local.
* **Requisitos / Detalhes:**
    * O sistema deve ser capaz de usar um modelo da OpenAI (e.g., `gpt-4o`) para o servi√ßo de gera√ß√£o quando configurado.
    * O sistema deve ser capaz de usar um modelo de embedding da OpenAI (e.g., `text-embedding-3-small`) para os processos de ingest√£o e consulta.
    * A chave da API da OpenAI deve ser lida a partir das configura√ß√µes (que por sua vez a l√™ de uma vari√°vel de ambiente).
    * A funcionalidade deve ser test√°vel de ponta a ponta.
* **Crit√©rios de Aceite (ACs):**
    * **AC 1:** Dado que `LLM_PROVIDER="openai"` e uma `OPENAI_API_KEY` v√°lida est√° configurada, quando eu envio uma `POST` para `/query`, ent√£o a resposta √© gerada com sucesso pela API da OpenAI.
    * **AC 2:** Dado que `EMBEDDING_PROVIDER="openai"` e uma `OPENAI_API_KEY` v√°lida est√° configurada, quando eu ingiro um novo documento via `POST` para `/ingest`, ent√£o os embeddings dos chunks s√£o gerados pela OpenAI e salvos corretamente no Neo4j.
    * **AC 3:** Dado que a `OPENAI_API_KEY` √© inv√°lida ou n√£o foi fornecida, quando o sistema tenta usar um servi√ßo da OpenAI, ent√£o ele retorna um erro claro e informativo (e.g., `500 Internal Server Error` com a causa raiz).
* **Defini√ß√£o de 'Pronto' (DoD Checklist):**
    * [ ] C√≥digo revisado por um par (PR aprovado).
    * [ ] Testes de integra√ß√£o que validam o fluxo com OpenAI (gera√ß√£o e embedding) foram criados e est√£o passando (podem necessitar de mocks para a API externa).
    * [ ] A depend√™ncia do cliente Python da OpenAI foi adicionada ao `requirements.txt`.
    * [ ] A documenta√ß√£o de configura√ß√£o (`configuration.md` ou `README.md`) foi atualizada para explicar como usar o provedor OpenAI.

#### Parte 2: Especifica√ß√£o T√©cnica (Vis√£o do Engenheiro)
* **Abordagem T√©cnica Proposta:**
    * Criar novas classes `OpenAIProvider` (para LLM) e `OpenAIEmbeddingProvider` que implementam as respectivas interfaces definidas na Hist√≥ria 1. Essas classes ir√£o encapsular as chamadas ao SDK da OpenAI.
* **Backend:**
    * - **Depend√™ncias:** Adicionar `openai` ao `requirements.txt`.
    * - **Implementa√ß√£o do LLM:**
        * Criar `src/generation/providers/openai.py`.
        * Implementar a classe `OpenAIProvider` herdando de `LLMProvider`.
        * O m√©todo `generate` ir√° instanciar o cliente da OpenAI, fazer a chamada para `client.chat.completions.create()` e formatar a resposta.
    * - **Implementa√ß√£o do Embedding:**
        * (Assumindo uma refatora√ß√£o similar para embeddings) Criar `src/retrieval/embedding_providers/openai.py`.
        * Implementar a classe `OpenAIEmbeddingProvider` que chama `client.embeddings.create()`.
    * - **Atualiza√ß√£o do Factory:** Adicionar a l√≥gica no factory de servi√ßos para instancionar as classes da OpenAI quando `settings.LLM_PROVIDER` ou `settings.EMBEDDING_PROVIDER` for `"openai"`.
* **Frontend:**
    * - N/A.
* **Banco de Dados:**
    * - Nenhuma altera√ß√£o de schema.
* **Quest√µes em Aberto / Riscos:**
    * - **Custo:** Chamadas √† API da OpenAI incorrem em custos. Os testes de integra√ß√£o devem usar mocks para evitar chamadas reais no pipeline de CI/CD.
    * - **Rate Limiting:** As APIs externas t√™m limites de taxa. Adicione controle de retry e deixe configuravel para evitar problemas.

---
### Hist√≥ria 3: Implementa√ß√£o do Provedor Google Gemini para Gera√ß√£o de Respostas
* **Tipo:** Funcional

#### Parte 1: Especifica√ß√£o Funcional (Vis√£o do Product Owner)
* **Hist√≥ria de Usu√°rio:** Como um Desenvolvedor, eu quero integrar os modelos do Google Gemini como uma op√ß√£o para gera√ß√£o de respostas, para que eu possa utilizar seus modelos avan√ßados e comparar performance/custo com outras solu√ß√µes dispon√≠veis.
* **Requisitos / Detalhes:**
    * O sistema deve ser capaz de usar um modelo do Google Gemini (e.g., `gemini-2.0-flash-exp`) para o servi√ßo de gera√ß√£o quando configurado.
    * A chave da API do Google deve ser lida a partir das configura√ß√µes (que por sua vez a l√™ de uma vari√°vel de ambiente).
    * A funcionalidade deve ser test√°vel de ponta a ponta.
    * O provider deve manter compatibilidade com a interface existente.
* **Crit√©rios de Aceite (ACs):**
    * **AC 1:** Dado que `LLM_PROVIDER="gemini"` e uma `GOOGLE_API_KEY` v√°lida est√° configurada, quando eu envio uma `POST` para `/query`, ent√£o a resposta √© gerada com sucesso pela API do Google Gemini.
    * **AC 2:** Dado que a `GOOGLE_API_KEY` √© inv√°lida ou n√£o foi fornecida, quando o sistema tenta usar o servi√ßo do Gemini, ent√£o ele retorna um erro claro e informativo (e.g., `500 Internal Server Error` com a causa raiz).
    * **AC 3:** Dado que o factory foi atualizado, quando o sistema √© configurado para usar Gemini, ent√£o a resposta √© gerada sem modificar outros componentes do sistema.
* **Defini√ß√£o de 'Pronto' (DoD Checklist):**
    * [ ] C√≥digo revisado por um par (PR aprovado).
    * [ ] Testes de integra√ß√£o que validam o fluxo com Google Gemini foram criados e est√£o passando (podem necessitar de mocks para a API externa).
    * [ ] A depend√™ncia do cliente Python do Google Generative AI foi adicionada ao `requirements.txt`.
    * [ ] A documenta√ß√£o de configura√ß√£o foi atualizada para explicar como usar o provedor Gemini.

#### Parte 2: Especifica√ß√£o T√©cnica (Vis√£o do Engenheiro)
* **Abordagem T√©cnica Proposta:**
    * Criar nova classe `GeminiProvider` que implementa a interface `LLMProvider` definida na arquitetura. Esta classe ir√° encapsular as chamadas ao SDK do Google Generative AI.
* **Backend:**
    * - **Depend√™ncias:** Adicionar `google-generativeai` ao `requirements.txt`.
    * - **Implementa√ß√£o do LLM:**
        * Criar `src/generation/providers/gemini.py`.
        * Implementar a classe `GeminiProvider` herdando de `LLMProvider`.
        * O m√©todo `generate_response` ir√° instanciar o cliente do Gemini, fazer a chamada para `model.generate_content()` e formatar a resposta.
        * Usar o modelo `gemini-2.0-flash-exp` conforme configurado.
    * - **Atualiza√ß√£o do Factory:** Atualizar a l√≥gica no factory de servi√ßos para instanciar a classe `GeminiProvider` quando `settings.LLM_PROVIDER` for `"gemini"`.
* **Frontend:**
    * - N/A.
* **Banco de Dados:**
    * - Nenhuma altera√ß√£o de schema.
* **Quest√µes em Aberto / Riscos:**
    * - **Custo:** Chamadas √† API do Google Gemini incorrem em custos. Os testes de integra√ß√£o devem usar mocks para evitar chamadas reais no pipeline de CI/CD.
    * - **Rate Limiting:** As APIs externas t√™m limites de taxa. Adicionar controle de retry e tratamento de erros configur√°vel.
    * - **Modelos Dispon√≠veis:** Verificar disponibilidade do modelo `gemini-2.0-flash-exp` e ter fallback se necess√°rio.

---
### Hist√≥ria 4: Sele√ß√£o Din√¢mica de Provider LLM via Interface e API
* **Tipo:** Funcional / UX

#### Parte 1: Especifica√ß√£o Funcional (Vis√£o do Product Owner)
* **Hist√≥ria de Usu√°rio:** Como um Usu√°rio do sistema RAG, eu quero poder escolher qual provider LLM usar para cada consulta individual (Ollama, OpenAI, ou Gemini), para que eu possa comparar respostas, otimizar custos por pergunta, e ter flexibilidade total sobre qual modelo usar sem precisar reconfigurar o sistema.
* **Requisitos / Detalhes:**
    * A interface Streamlit deve ter um seletor visual para escolher o provider antes de fazer uma pergunta.
    * A API deve aceitar um par√¢metro opcional `provider` no request para sobrescrever a configura√ß√£o padr√£o.
    * O sistema deve mostrar qual provider foi usado na resposta (tanto na UI quanto na API).
    * Deve haver fallback para o provider padr√£o caso o especificado n√£o esteja dispon√≠vel.
    * A funcionalidade deve permitir compara√ß√£o lado a lado entre providers.
* **Crit√©rios de Aceite (ACs):**
    * **AC 1:** Dado que estou na interface Streamlit, quando eu seleciono "OpenAI" no dropdown de providers e fa√ßo uma pergunta, ent√£o a resposta √© gerada usando OpenAI e o sistema indica claramente qual provider foi usado.
    * **AC 2:** Dado que envio um `POST` para `/query` com `{"question": "teste", "provider": "gemini"}`, quando o sistema processa a requisi√ß√£o, ent√£o a resposta √© gerada usando Gemini independente da configura√ß√£o padr√£o do arquivo `.env`.
    * **AC 3:** Dado que especifico um provider inv√°lido (e.g., `"anthropic"`), quando o sistema processa a requisi√ß√£o, ent√£o ele retorna um erro claro listando os providers dispon√≠veis.
    * **AC 4:** Dado que especifico um provider que requer API key n√£o configurada, quando o sistema tenta usar esse provider, ent√£o retorna erro informativo sobre a configura√ß√£o necess√°ria.
* **Defini√ß√£o de 'Pronto' (DoD Checklist):**
    * [ ] Interface Streamlit atualizada com seletor de provider.
    * [ ] API aceita par√¢metro `provider` opcional.
    * [ ] Respostas incluem indica√ß√£o do provider usado.
    * [ ] Testes cobrindo todos os cen√°rios de sele√ß√£o din√¢mica.
    * [ ] Documenta√ß√£o atualizada explicando a nova funcionalidade.

#### Parte 2: Especifica√ß√£o T√©cnica (Vis√£o do Engenheiro)
* **Abordagem T√©cnica Proposta:**
    * Modificar o modelo `QueryRequest` para incluir campo opcional `provider`.
    * Criar fun√ß√£o `create_llm_provider_dynamic()` que aceita provider como par√¢metro.
    * Atualizar interface Streamlit com `st.selectbox` para escolha de provider.
    * Implementar valida√ß√£o e tratamento de erros para providers indispon√≠veis.
* **Backend:**
    * - **Modelos de API:**
        * Atualizar `src/models/api_models.py`:
            * Adicionar `provider: Optional[Literal["ollama", "openai", "gemini"]] = None` em `QueryRequest`.
            * Adicionar `provider_used: str` em `QueryResponse`.
    * - **L√≥gica de Sele√ß√£o:**
        * Modificar `src/generation/generator.py`:
            * Criar `create_llm_provider_dynamic(provider_override: Optional[str] = None)`.
            * Implementar l√≥gica de fallback e valida√ß√£o.
        * Atualizar endpoint `/query` para usar o novo par√¢metro.
    * - **Valida√ß√£o:**
        * Implementar valida√ß√£o de provider dispon√≠vel.
        * Verificar configura√ß√£o de API keys necess√°rias.
        * Retornar erros informativos com lista de providers v√°lidos.
* **Frontend (Streamlit):**
    * - **Seletor de Provider:**
        * Adicionar `st.selectbox` com op√ß√µes: ["Auto (padr√£o)", "Ollama", "OpenAI", "Gemini"].
        * Mostrar status de configura√ß√£o (üü¢ configurado, üî¥ n√£o configurado) ao lado de cada op√ß√£o.
    * - **Indica√ß√£o de Provider Usado:**
        * Exibir badge ou tag mostrando qual provider gerou a resposta.
        * Adicionar m√©tricas de tempo de resposta por provider.
    * - **Modo Compara√ß√£o (Futuro):**
        * Interface para comparar respostas de m√∫ltiplos providers lado a lado.
* **Banco de Dados:**
    * - Nenhuma altera√ß√£o necess√°ria.
* **Quest√µes em Aberto / Riscos:**
    * - **Performance:** Chamadas para m√∫ltiplos providers simultaneamente podem impactar performance.
    * - **Custo:** Facilitar mudan√ßa de provider pode aumentar uso inadvertido de APIs pagas.
    * - **UX:** Interface deve ser clara sobre quais providers est√£o configurados e funcionais.
    * - **Caching:** Considerar se cache de respostas deve ser por provider ou compartilhado.
