# Hist√≥ria 7: Upload de Documento para Infer√™ncia de Schema

## üéØ Objetivo
Criar um sistema que permita upload de documentos, extra√ß√£o e armazenamento tempor√°rio do texto em mem√≥ria, e gera√ß√£o de chave de consulta para infer√™ncia de schema, eliminando a necessidade de passar texto diretamente na API.

## üìã Contexto
Atualmente, o endpoint `/api/v1/schema/infer` requer que o texto seja enviado diretamente no payload JSON. Isso apresenta limita√ß√µes:
- **Seguran√ßa**: Textos sens√≠veis trafegam pela rede
- **Tamanho**: Limita√ß√µes de payload para documentos grandes
- **UX**: Usu√°rio precisa extrair texto manualmente
- **Logs**: Textos podem aparecer em logs de sistema

## üéØ Casos de Uso
1. **Upload Seguro**: Fazer upload de documento (.txt/.pdf) sem expor conte√∫do
2. **An√°lise Pr√©via**: Analisar schema antes da ingest√£o completa
3. **Documentos Grandes**: Processar arquivos maiores que limite de JSON
4. **Reutiliza√ß√£o**: Usar mesma chave para m√∫ltiplas an√°lises
5. **Auditoria**: Rastrear an√°lises sem expor conte√∫do

## üìù Hist√≥ria do Usu√°rio
**Como** desenvolvedor integrador do sistema RAG  
**Eu quero** fazer upload de um documento e receber uma chave tempor√°ria  
**Para que** eu possa usar essa chave para inferir schema sem expor o texto  
**E** ter controle sobre quando o documento √© removido da mem√≥ria

## ‚úÖ Crit√©rios de Aceita√ß√£o

### AC1: Endpoint de Upload para Schema
- **Dado** que existe um endpoint `POST /api/v1/schema/upload`
- **Quando** envio um arquivo (.txt ou .pdf)
- **Ent√£o** recebo uma chave √∫nica (UUID) e metadados do arquivo
- **E** o texto √© extra√≠do e armazenado temporariamente em mem√≥ria

### AC2: Infer√™ncia via Chave
- **Dado** que tenho uma chave v√°lida de um upload
- **Quando** chamo `POST /api/v1/schema/infer` com a chave
- **Ent√£o** recebo o schema inferido sem precisar enviar o texto
- **E** posso configurar `max_sample_length`

### AC3: Gest√£o de Mem√≥ria
- **Dado** que documentos s√£o armazenados em mem√≥ria
- **Quando** um documento n√£o √© usado por 30 minutos
- **Ent√£o** ele √© automaticamente removido da mem√≥ria
- **E** tentativas de uso retornam erro 404

### AC4: Endpoint de Limpeza Manual
- **Dado** que tenho uma chave v√°lida
- **Quando** chamo `DELETE /api/v1/schema/documents/{key}`
- **Ent√£o** o documento √© removido imediatamente da mem√≥ria
- **E** tentativas subsequentes retornam erro 404

### AC5: Listagem de Documentos em Mem√≥ria
- **Dado** que existem documentos em mem√≥ria
- **Quando** chamo `GET /api/v1/schema/documents`
- **Ent√£o** recebo lista de chaves ativas com metadados
- **E** informa√ß√µes de tempo de expira√ß√£o

### AC6: Valida√ß√£o e Seguran√ßa
- **Dado** que fa√ßo upload de arquivo inv√°lido
- **Quando** o sistema processa a requisi√ß√£o
- **Ent√£o** recebo erro 415 (tipo n√£o suportado)
- **E** nenhum dado √© armazenado em mem√≥ria

## üîß Especifica√ß√£o T√©cnica

### 1. Upload de Documento
```http
POST /api/v1/schema/upload
Content-Type: multipart/form-data

file: document.pdf
```

**Response (201):**
```json
{
  "key": "550e8400-e29b-41d4-a716-446655440000",
  "filename": "document.pdf",
  "size_bytes": 1048576,
  "text_length": 15847,
  "created_at": "2025-01-15T10:30:00Z",
  "expires_at": "2025-01-15T11:00:00Z",
  "file_type": "pdf"
}
```

### 2. Infer√™ncia via Chave
```http
POST /api/v1/schema/infer
Content-Type: application/json

{
  "document_key": "550e8400-e29b-41d4-a716-446655440000",
  "max_sample_length": 1000
}
```

**Response (200):**
```json
{
  "node_labels": ["Person", "Company", "Technology"],
  "relationship_types": ["WORKS_AT", "USES", "DEVELOPS"],
  "source": "llm",
  "model_used": "qwen3:8b",
  "processing_time_ms": 1250.5,
  "document_info": {
    "filename": "document.pdf",
    "text_length": 15847,
    "sample_used": 1000
  }
}
```

### 3. Listagem de Documentos
```http
GET /api/v1/schema/documents
```

**Response (200):**
```json
{
  "documents": [
    {
      "key": "550e8400-e29b-41d4-a716-446655440000",
      "filename": "document.pdf",
      "size_bytes": 1048576,
      "text_length": 15847,
      "created_at": "2025-01-15T10:30:00Z",
      "expires_at": "2025-01-15T11:00:00Z",
      "last_accessed": "2025-01-15T10:35:00Z"
    }
  ],
  "total_documents": 1,
  "memory_usage_mb": 15.2
}
```

### 4. Remo√ß√£o de Documento
```http
DELETE /api/v1/schema/documents/550e8400-e29b-41d4-a716-446655440000
```

**Response (200):**
```json
{
  "message": "Document removed successfully",
  "key": "550e8400-e29b-41d4-a716-446655440000"
}
```

## üèóÔ∏è Arquitetura de Implementa√ß√£o

### Cache Service (Novo)
```python
class DocumentCacheService:
    def __init__(self):
        self._cache: Dict[str, CachedDocument] = {}
        self._cleanup_task = None
    
    async def store_document(self, file_content: bytes, filename: str) -> str
    async def get_document(self, key: str) -> Optional[CachedDocument]
    async def remove_document(self, key: str) -> bool
    async def list_documents(self) -> List[DocumentInfo]
    async def cleanup_expired(self) -> int
```

### Modelos de Dados
```python
@dataclass
class CachedDocument:
    key: str
    filename: str
    text_content: str
    file_type: str
    size_bytes: int
    created_at: datetime
    last_accessed: datetime
    expires_at: datetime

class SchemaUploadResponse(BaseModel):
    key: str
    filename: str
    size_bytes: int
    text_length: int
    created_at: datetime
    expires_at: datetime
    file_type: str

class SchemaInferByKeyRequest(BaseModel):
    document_key: str
    max_sample_length: Optional[int] = 500
```

### Endpoints Implementados
1. `POST /api/v1/schema/upload` - Upload e cache
2. `POST /api/v1/schema/infer` - Infer√™ncia (suporta texto OU chave)
3. `GET /api/v1/schema/documents` - Listar cache
4. `DELETE /api/v1/schema/documents/{key}` - Remover do cache

## üîí Considera√ß√µes de Seguran√ßa

### Gest√£o de Mem√≥ria
- **TTL**: 30 minutos por padr√£o (configur√°vel)
- **Cleanup**: Task autom√°tica a cada 5 minutos
- **Limite**: M√°ximo 100 documentos em cache simult√¢neo
- **Tamanho**: M√°ximo 50MB por documento

### Prote√ß√£o de Dados
- **UUID seguro**: Chaves n√£o-sequenciais geradas via `uuid4()`
- **Sem persist√™ncia**: Dados apenas em mem√≥ria (n√£o em disco)
- **Logs seguros**: Chaves loggadas, conte√∫do jamais
- **Rate limiting**: M√°ximo 10 uploads por minuto por IP

### Valida√ß√£o
- **Tipos permitidos**: Apenas .txt e .pdf
- **Tamanho m√°ximo**: 50MB por arquivo
- **Sanitiza√ß√£o**: Valida√ß√£o rigorosa de content-type

## üß™ Plano de Testes

### Testes Unit√°rios
1. **DocumentCacheService**
   - Store/retrieve/remove operations
   - TTL e cleanup autom√°tico
   - Gest√£o de mem√≥ria

2. **Modelos Pydantic**
   - Valida√ß√£o de uploads
   - Serializa√ß√£o/deserializa√ß√£o
   - Edge cases

### Testes de Integra√ß√£o
1. **Upload Workflow**
   - Upload ‚Üí Cache ‚Üí Infer ‚Üí Cleanup
   - Diferentes tipos de arquivo
   - Erro handling

2. **Memory Management**
   - Expira√ß√£o autom√°tica
   - Limpeza manual
   - Limites de capacidade

### Testes de Performance
1. **Concorr√™ncia**: M√∫ltiplos uploads simult√¢neos
2. **Mem√≥ria**: Consumo com documentos grandes
3. **Cleanup**: Performance da limpeza autom√°tica

## üìä Defini√ß√£o de Pronto (DoD)
- [ ] DocumentCacheService implementado
- [ ] 4 novos endpoints funcionando
- [ ] Modelos Pydantic criados
- [ ] Cleanup autom√°tico funcionando
- [ ] Testes unit√°rios > 95% cobertura
- [ ] Testes de integra√ß√£o passando
- [ ] Documenta√ß√£o completa
- [ ] Limite de mem√≥ria respeitado
- [ ] Rate limiting implementado
- [ ] Logs de seguran√ßa configurados

## üîÑ Metodologia
- **TDD**: Testes primeiro
- **Security-first**: Prote√ß√£o de dados sens√≠veis
- **Backward compatible**: Endpoint existente continua funcionando
- **Memory-efficient**: Gest√£o cuidadosa de recursos

## üí° Benef√≠cios Esperados
1. **Seguran√ßa**: Texto n√£o trafega em APIs REST
2. **Performance**: Reutiliza√ß√£o de extra√ß√µes caras
3. **UX**: Upload simples de arquivos
4. **Escalabilidade**: Suporte a documentos grandes
5. **Auditoria**: Rastreabilidade sem exposi√ß√£o de dados

## üéØ Fluxo de Uso

### Cen√°rio T√≠pico
1. **Upload**: `POST /schema/upload` ‚Üí recebe chave
2. **An√°lise**: `POST /schema/infer` com chave ‚Üí schema
3. **Decis√£o**: Se satisfeito, procede com ingest√£o real
4. **Limpeza**: `DELETE /schema/documents/{key}` ou expira automaticamente

### Integra√ß√£o com Ingest√£o Existente
```mermaid
graph LR
    A[Upload Doc] --> B[Get Key]
    B --> C[Infer Schema]
    C --> D{Schema OK?}
    D -->|Yes| E[Real Ingest]
    D -->|No| F[Adjust & Retry]
    E --> G[Auto Cleanup]
    F --> C
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente
```env
# Schema Cache Settings
SCHEMA_CACHE_TTL_MINUTES=30
SCHEMA_CACHE_MAX_DOCUMENTS=100
SCHEMA_CACHE_MAX_SIZE_MB=50
SCHEMA_CACHE_CLEANUP_INTERVAL_MINUTES=5

# Rate Limiting
SCHEMA_UPLOAD_RATE_LIMIT=10  # uploads per minute per IP
```

Esta implementa√ß√£o oferece um sistema robusto, seguro e eficiente para an√°lise de schema sem comprometer a seguran√ßa dos dados!